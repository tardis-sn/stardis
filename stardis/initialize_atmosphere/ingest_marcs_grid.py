#!/usr/bin/env python
# coding: utf-8

import numpy as np
import pandas as pd

from scipy.interpolate import LinearNDInterpolator

from glob import glob
import gzip
from tqdm.notebook import tqdm



def slice_grid_to_hdf_from_dir(storage_dir,  output_hdf, min_teff=0., max_teff=100000.):
    """
    Create an hdf file from a directory of Marcs models. 
    The hdf uses the list of parameters as a key.


    Keyword arguments:
    storage_dir -- the directory that contains the grid of marcs models 
    output_hdf -- name of the hdf file to write to
    min_teff -- don't process files below this temperature to write to the hdf (default 0)
    max_teff -- don't process files above this temperature to write to the hdf (default 1e5)
    """    

    
    s = pd.HDFStore(output_hdf)    
    all_paths = glob(storage_dir + '/*.gz')
    
    for i, fpath in tqdm(enumerate(all_paths)):
        with gzip.open(fpath, 'r') as file:
            contents = file.readlines()
            fname, line_teff, line_flux, line_grav, line_turb, line_mass, line_feh, line_rad, line_lum, line_alpha_nu_y_beta, line_X_Y_Z, *other = contents
            
            teff = float(line_teff.decode().split()[0])
            logg = np.log10(float(line_grav.decode().split()[0]))
    
            feh = float(line_feh.decode().split()[0])
            ah = float(line_feh.decode().split()[1])
            turb = float(line_turb.decode().split()[0])
    #        X = float(line_X_Y_Z.decode().split()[0])
    #        Y = float(line_X_Y_Z.decode().split()[1])
    #        Z = float(line_X_Y_Z.decode().split()[2])
    
            file.close()

            #prep key
            file_key = 'teff_' + str(teff) + '_logg_' + str(logg) + '_feh_' + str(feh) + '_ah_' + str(ah) + '_turb_' + str(turb)
            
            if teff < min_teff:
                continue
            if teff > max_teff:
                continue            
                
            marcs_model1 = pd.read_csv(
                fpath, skiprows=24, nrows=56, delim_whitespace=True, index_col="k"
                )
            marcs_model2 = pd.read_csv(
                fpath, skiprows=81, nrows=56, index_col="k", sep='(?:\s+)|(?<=\+\d{2})(?=-)'
                )
            del marcs_model2["lgTauR"]
            marcs_model = marcs_model1.join(marcs_model2)
            marcs_model.columns = [item.lower() for item in marcs_model.columns]
            
            s[file_key] = marcs_model
        
    s.close()




class MarcsGrid(object):
    """
    Class to read in a grid of Marcs models stored in an HDF, and hold an interpolator that can be evaluated to return the contents
    of a Marcs simulation file with the information necessary to run stardis. Also holds metadata for the grid. 

    Should be initialized with an hdf file generated by the slice_grid_to_hdf_from_dir function in this python file. 
    """

    
    #This object needs to load in the hdf5 written by the slice_grid method, create an interpolated object from the contents, and evaluate at a specific set of parameters
    #attrs should show bounds of the interpolation
    
    def __init__(self, fname, teff_start=0, teff_stop=np.inf, logg_start=0, logg_stop=np.inf, feh_start=-np.inf, feh_stop=np.inf,
                 ah_start=-np.inf, ah_stop=np.inf, turb_start=0, turb_stop=np.inf):
        
        self.teff_start = teff_start
        self.teff_stop = teff_stop
        self.logg_start = logg_start
        self.logg_stop = logg_stop
        self.feh_start = feh_start
        self.feh_stop = feh_stop
        self.ah_start = ah_start
        self.ah_stop = ah_stop
        self.turb_start = turb_start
        self.turb_stop = turb_stop
        
        self.interpolated_grid, self.output_metadata = self.load_from_hdf(fname)

        
        
    def bounds(self):
        pass

    
    def load_from_hdf(self, hdf):
        #load the hdf in to an array of parameters and and array of full sets of values to be interpolated between

        #Initializing as normal arrays. This should really be fixed to initialize as numpy arrays of correct size and shape
        #so that we don't have to keep reallocating memory with appends. 
        params_arr = []
        values_arr = []
        
        print('Reading Marcs models from hdf')
        with pd.HDFStore(hdf, mode = 'r') as hfile:
            for key in tqdm(hfile.keys()):
                t, teff, l, logg, f, feh, a, ah, tu, turb = key.split('_')
                params = [float(teff), float(logg), float(feh), float(ah), float(turb)]
                df = hfile[key]
        
                params_arr.append(params)
                values_arr.append(df.values)
        
        num_arr = np.array(params_arr)
        self.teff_start = num_arr[:,0].min()
        self.teff_stop = num_arr[:,0].max()
        self.logg_start = num_arr[:,1].min()
        self.logg_stop = num_arr[:,1].max()
        self.feh_start = num_arr[:,2].min()
        self.feh_stop = num_arr[:,2].max()
        self.ah_start = num_arr[:,3].min()
        self.ah_stop = num_arr[:,3].max()        
        self.turb_start = num_arr[:,4].min()
        self.turb_stop = num_arr[:,4].max()   
        
        print('Interpolating')
        interpolated = LinearNDInterpolator(params_arr, values_arr)
        return(interpolated, df.columns)
        
        
    def evaluate(self, teff, logg, feh, ah, turb):
        """
        Evaluate the interpolator at a given teff, logg, feh, ah, and turb.
        Returns the contents of a Marcs simulation file with all of the necessary information to run stardis.
        Returns nans if outside the range. 
        """
        #evaluate interpolated object at given params
        return(self.interpolated_grid(teff, logg, feh, ah, turb))
        


